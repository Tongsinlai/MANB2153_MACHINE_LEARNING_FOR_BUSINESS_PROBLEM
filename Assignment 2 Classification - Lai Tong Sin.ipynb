{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 Assessment\n",
    "## 1. From the exercises in Data Camp, demonstrate on how you can use kNN, Decision Tree and Naive Bayes to perform classification and regression, on Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Prediction: ['Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-virginica' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-versicolor']\n",
      "KNN Classifier Accuracy:0.9555555555555556\n",
      "Decision Tree Prediction: ['Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-virginica' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor']\n",
      "Decision Tree Accuracy:0.9777777777777777\n",
      "NB Classifier Prediction: ['Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-setosa' 'Iris-virginica' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-virginica'\n",
      " 'Iris-versicolor']\n",
      "NB Classifier Accuracy:0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "# Import KNeighborsClassifier from sklearn.neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Define csv path (#change path if script and the csv are not in the same folder)\n",
    "df=pd.read_csv('C:/Users/origi/Documents/UTM/Machine Learning/iris.csv') \n",
    "# Create arrays for the features and the response variable\n",
    "y = df['iris'].values\n",
    "X = df.drop('iris', axis=1).values\n",
    "\n",
    "#Split train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42, stratify=y) \n",
    "\n",
    "# Create a k-NN classifier with 6 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "# Predict and print the accuracy for knn classifier\n",
    "new_prediction = knn.predict(X_test)\n",
    "print(\"KNN Classifier Prediction: {}\".format(new_prediction))\n",
    "print(\"KNN Classifier Accuracy:{}\".format(knn.score(X_test,y_test)))\n",
    "\n",
    "# Create Decision Tree model\n",
    "dt = tree.DecisionTreeClassifier(criterion=\"gini\")\n",
    "\n",
    "# Fit DT model\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# DT model result and accuracy\n",
    "new_prediction = dt.predict(X_test)\n",
    "print(\"Decision Tree Prediction: {}\".format(new_prediction))\n",
    "print(\"Decision Tree Accuracy:{}\".format(dt.score(X_test,y_test)))\n",
    "\n",
    "# Create Naive Bayes model\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Fit NB model\n",
    "nb.fit(X_train,y_train)\n",
    "\n",
    "# NB model result and accuracy\n",
    "new_prediction = knn.predict(X_test)\n",
    "print(\"NB Classifier Prediction: {}\".format(new_prediction))\n",
    "print(\"NB Classifier Accuracy:{}\".format(nb.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing exercises: find 10 papers that have applied kNN, Decision Tree and Naive Bayes to solve their problems. For each paper, explain the problems they are solving, the techniques and the data that were used in the paper. Write this in your Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "a. Chang Liu and his team have developed a tool to forecast the copper price without human bias. They utilized decision tree algorithm to synthesis the underlying relationship between copper prices, energy cost, crude oil prices, alternative metal prices. In their study, they used a dataset that consists of prices for crude oil, gold, silver, coffee, lean hogs, Dow Jones Index, past and current copper prices. Hence, the dataset contains of 7 features and 1 desired output. Then they use decision tree algorithm model to predict the price accurately and consistently. They have achieved the mean absolute percentage errors down to 4% and demonstrated its robustness and prone from human bias.\n",
    "\n",
    "b. Han-joon and his team is working on an improved method on text meaning where usually training with pre-labelled class or bag-of-words. They planned to incorporate semantic text mining which analyze the meaning of the words along with statistics feature terms. Hence they chose Naive Bayes as their algorithm due to its simplicity and utilized probability calculation to predict the output. By training and testing with massive datas from Reuters, 20Newsgroup and OHSUMED corpora, they are able to prove this algorithm perform much superior towards deep learning algorithms.  \n",
    "\n",
    "c. Adeniya and his team wrote an article on data mining and recommendation system using kNN classifier to filter out informations in the internet automatically. This system learns the users' behaviours through their clicks and recommend any future contents that they might be interested and improve the user experience. They used kNN algorithm due to its simplicity and track records on web mining to classifier all the web contents so that the system can recommend the users the content they need. In the article, they are able to develop this system and shows that kNN classifier is transparent and straightforward. \n",
    "\n",
    "d. Rabeb and his team decided to approach in solving a problem of cross selectivity of gases in electronic nose using combination of SVM and kNN algorithm. Three sensors in E-nose system were used to detect and record down three types of gases which are ozone, ethanol and acetone. Both SVM and kNN algorithm used these parameters and predict the correct gases. The accuracy for kNN algorithm is up 89% compared to SVM algorithm which are between 75 - 80%.\n",
    "\n",
    "e. Benjamin and his team were developing a predictor on service industry performance by using decision tree analysis. By using data on global service industry provided from World Bank, they have framed their decision tree algorithm by the technology, organization and environment features. They found that company financial strength is one of the strongest predictor and market expansion actually negatively affect the revenue growth for the Asia and Africa countries. As a result, they are able to show companies or policy makers on making better decision.\n",
    "\n",
    "f. Sanaz and team are developing a method involves two stages of credit card payment fraud detection. The first stage of the detection extract the users spending behaviour and transaction information as their features, then passing to second stage of detection which employed random forest and kNN classifier to evaluate whether certain transaction is legitimate or fraudulent.  As a result, they see an increase of 23% on prevention of damage from fraudulent transaction. \n",
    "\n",
    "g. Roman and his team compared multiple algorithms like linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), soft independent modeling of class analogy (SIMCA), partial least square (PLS), k nearest neighbor (kNN), support vector machine (SVM), probability neural network (PNN) and multilayer perceptron (MLP) on classifying gasoline quality. In their study, they found kNN, SVM and PNN are the best predictor among the rest of algorithms while MLP perform poorly despite they hypothesize MLP as most efficient.\n",
    "\n",
    "h. Verma, Liu and Kevin present their research on a new colour space oRGB-SIFT desriptor which used for computer vision and object recognition. They integrate this new descriptor with RGB, Greyscale SIFT to form Color SIFT Fusion (CSF) and Color Greyscale SIFT Fusion (CGSF) for image classification with special applications to biometrics.  They evaluate this SIFT descriptor by running through 20 images from Caltech-256 and UPOL Iris database. As result, oRGB scored 62.75 while CSF scored 71.3 and CGSF scored 75.5.\n",
    "\n",
    "i. Ashkan and his team is developing an approach to detect unbalanced fault in rotating machines using kNN and SVM classifier. They developed a fault diagnosis approach that classify unbalanced condition based on multiple features on vibration, load conditions and others. In total there are 29 features parameters used in this fault diagnosis approach. SVM approach scored the highest on 95.87% while kNN approach only managed to get 77.51%.\n",
    "\n",
    "j. Freeman developed a model based on Naive Bayes algorithm to detect spammy or fake names in social media. His purpose is to increase user experience in social media and prevent future abusive or scam activities. Naive Bayes algorithm helped to analyse the clickstream and social graph history. He used LinkedIn member data to train and test his model and achieved AUC of 0.85 on test set. This model also consists of 3.3% false positive rate which is improved from its previous model (7.0%). \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> \n",
    "References\n",
    "1. C. Liu, Z. Hu, Y. Li, S. Liu. (2017). Forecasting copper prices by decision tree learning.\n",
    "2. H. Kim, J Kim, J Kim, P Lim. (2018). Towards perfect text classification with Wikipedia-based semantic Naïve Bayes learning.\n",
    "3. D.A. Adeniyi, Z. Wei, Y. Yongquan (2016). Automated web usage data mining and recommendation system using K-Nearest Neighbor (KNN) classification method.\n",
    "4. R. Faleh, S. Gomri, M. Othman, K. Aguir, A. Kachouri (2017). Enhancing WO3 gas sensor selectivity using a set of pollutant detection classifiers.\n",
    "5. B. Yeo, D. Grant (2017). Predicting service industry performance using decision tree analysis\n",
    "6. S. Nami, M. Shajari (2018). Cost-sensitive payment card fraud detection based on dynamic random forest and k-nearest neighbors.\n",
    "7. R.M. Balabin, R.Z. Safieva, E.I. Lomakina (2010). Gasoline classification using near inflared (NIR) spectroscopy data: Comparison of multivariate techniques.\n",
    "8. A. Verma, C. Liu, J. Jia (2011). New colour SIFT descriptors for image classification with applications to biometrics.\n",
    "9. A. Moosavian, H. Ahmadi, B. Sakhaei, R. Labbafi (2014). Support vector machine and K-nearest neighbour for unbalanced fault detection.\n",
    "10. D.M. Freeman (2013). Using Naive Bayes to detect spammy names in social networks.\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
